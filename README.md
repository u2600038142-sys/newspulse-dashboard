## ğŸ‘¤ Author

**Markus Lasroha Oppusunggu**  
- ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/markus-oppusunggu/)  
- ğŸ’» [GitHub](https://github.com/u2600038142-sys)  
- âœ‰ï¸ [Email](mailto:oppusunggu.markus@gmail.com)  

# ğŸ“Š NewsPulse Dashboard

End-to-end **data pipeline & analytics** project on **Microsoft Azure**.  
Scrape global news â†’ cleanse â†’ load to a SQL data warehouse â†’ visualize in Power BI â†’ publish via GitHub Pages.

**Live demo:** https://u2600038142-sys.github.io/newspulse-dashboard/

---

## âš¡ Architecture

![Architecture](Docs/Architecture.png)

**Flow (high-level):**
1. **Scraping** â€” UiPath fetches Reuters â€œWorldâ€ news (past 24h) â†’ CSV.
2. **Landing Zone** â€” CSV stored in Azure Blob (`raw/`).
3. **Cleansing** â€” Azure Databricks (PySpark) cleans multipart noise â†’ valid CSV in `clean/`.
4. **Orchestration** â€” Azure Data Factory:
   - Truncate staging (SQL SP)
   - Run Databricks notebook (cleansing)
   - Copy clean CSV â†’ Azure SQL staging
   - Load DWH via stored procedure
5. **Data Warehouse** â€” Azure SQL (star schema: `dim_date`, `dim_source`, `fact_article`).
6. **Visualization** â€” Power BI connected to DWH.
7. **Publishing** â€” Power BI report embedded in `index.html` (GitHub Pages).
8. **NewsSummarywithAI** - Azure AI Foundry with OpenAI GPT4 model.

---

## ğŸ› ï¸ Tech Stack

- **UiPath** â€” News scraping automation  
- **Azure Blob Storage** â€” Raw & clean file store  
- **Azure Databricks (PySpark)** â€” Data cleansing/transforms  
- **Azure Data Factory** â€” Orchestration & scheduling  
![ADF](Docs/ADF.png)
- **Azure SQL Database** â€” Staging & DWH (star schema)  
- **Power BI** â€” Interactive dashboard  
- **GitHub Pages** â€” Public web hosting
- **Azure AI Foundry** - Creating AI for news summary

---

## ğŸš€ Pipeline Details

### 1) Ingestion (UiPath â†’ Blob)
- Output CSV schema: `source_name, source_url, title`
- Path: `raw/ingest_date=YYYY-MM-DD/source=reuters/*.csv`

### 2) Cleansing (Databricks, PySpark)
- Read as **text** â†’ filter multipart boundaries / headers
- Reparse for a CSV valid (quote support, multiline)
- save to `clean/ingest_date=YYYY-MM-DD/source=reuters/clean_reuters.csv`

### 3) Orchestration (ADF)
Activities (sequences):
1. **Stored Procedure** â†’ `dbo.sp_truncate_staging` (atau `sp_clear_staging_by_date`)
2. **Databricks Notebook** â†’ `nb_clean_news` (params: `p_date`, `p_source`)
3. **Copy Data** â†’ Blob `clean/*.csv` â†’ SQL `dbo.staging_fact_article`
4. **Stored Procedure** â†’ `dwh.sp_load_articles_from_staging` (load DWH)

### 4) Data Warehouse (Azure SQL)
Schema `dwh`:
- `dim_date(date_key PK, full_date, year, month, month_name, day, weekday_name)`
- `dim_source(source_id PK, source_name, source_url)`
- `fact_article(article_id PK, date_key FK, source_id FK, title, load_ts)`

### 5) AI News Summary (Azure AI Foundry)
Model OpenAI GPT4

Indexing:
```sql
-- Staging
CREATE INDEX IX_staging_ingestdate ON dbo.staging_fact_article(ingest_date);

-- DWH
CREATE UNIQUE INDEX UX_dim_source_name_url ON dwh.dim_source(source_name, source_url);
CREATE INDEX IX_fact_article_date_source ON dwh.fact_article(date_key, source_id);