{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6111651d-bf1f-46d9-8fe6-7d8d7ca7f025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db88255f-3263-41b7-ac27-b8b5c3b130ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 0) Imports & Parameters (SAFE after restart)\n",
    "import datetime, time\n",
    "from typing import List, Tuple\n",
    "from openai import AzureOpenAI\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# helper bikin/ambil widget text dgn default\n",
    "def ensure_widget_text(name: str, default: str) -> str:\n",
    "    try:\n",
    "        # kalau belum ada, create\n",
    "        dbutils.widgets.get(name)\n",
    "    except:\n",
    "        dbutils.widgets.text(name, default)\n",
    "    return dbutils.widgets.get(name)\n",
    "\n",
    "# widget tanggal (UTC); default = hari ini\n",
    "p_date_str = ensure_widget_text(\"p_date\", \"\")\n",
    "if not p_date_str:\n",
    "    p_date_str = datetime.datetime.utcnow().date().isoformat()\n",
    "\n",
    "# widget minimal judul per source; default = \"1\"\n",
    "p_min_titles_str = ensure_widget_text(\"p_min_titles\", \"1\")\n",
    "try:\n",
    "    p_min_titles = int(p_min_titles_str)\n",
    "except ValueError:\n",
    "    p_min_titles = 1  # fallback aman\n",
    "\n",
    "print(f\"Params → p_date={p_date_str}, p_min_titles={p_min_titles}\")\n",
    "p_date=p_date_str\n",
    "p_min_titles=p_min_titles\n",
    "\n",
    "# ----------------\n",
    "# 1) AOAI Client\n",
    "# ----------------\n",
    "SCOPE = \"kv-newspulse\"  # secret scope (Key Vault–backed)\n",
    "\n",
    "endpoint    = dbutils.secrets.get(SCOPE, \"aoai-endpoint1\")\n",
    "api_key     = dbutils.secrets.get(SCOPE, \"aoai-subscription-key1\")\n",
    "deployment  = dbutils.secrets.get(SCOPE, \"aoai-deployment1\")\n",
    "api_version = \"2024-12-01-preview\"  # gunakan yang sama seperti di Azure AI Foundry (View code)\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint\n",
    ")\n",
    "\n",
    "# Sanity check (ringkas 1 kalimat)\n",
    "test = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Tes singkat: balas 'OK'.\"}],\n",
    "    max_tokens=5, temperature=0\n",
    ")\n",
    "print(\"AOAI OK:\", test.choices[0].message.content.strip())\n",
    "\n",
    "# ------------------------\n",
    "# 2) Azure SQL JDBC CONFIG\n",
    "# ------------------------\n",
    "# Ganti sesuai environment kamu\n",
    "JDBC_URL = \"jdbc:sqlserver://sql-newspulse-dev.database.windows.net:1433;databaseName=sqldb-newspulse-dev\"\n",
    "SQL_USER = \"sqladmin\"\n",
    "SQL_PWD  = \"Danone.20091989\"   # (sementara hardcoded sesuai pilihanmu)\n",
    "JDBC_PROPS = {\n",
    "    \"user\": SQL_USER,\n",
    "    \"password\": SQL_PWD,\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "TABLE_STAGING = \"dbo.staging_collect_all_fact_article\"\n",
    "TABLE_SUMMARY = \"dwh.fact_article_summary\"\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Load artikel untuk p_date\n",
    "# -------------------------------\n",
    "articles_df = (\n",
    "    spark.read.jdbc(JDBC_URL, TABLE_STAGING, properties=JDBC_PROPS)\n",
    "         .filter(F.to_date(\"load_ts\") == F.lit(str(p_date)))\n",
    "         .select(\"source_name\",\"Url\",\"title\",\"load_ts\")\n",
    ")\n",
    "\n",
    "print(\"Articles today:\", articles_df.count())\n",
    "display(articles_df.limit(10))\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4) Helper: Summarize batch via Azure OpenAI\n",
    "# ------------------------------------------\n",
    "def summarize_batch(source: str, items: List[dict], day_iso: str,\n",
    "                    max_titles:int=50, max_tokens:int=600, temperature:float=0.3,\n",
    "                    retries:int=3, backoff:int=2) -> Tuple[str, str]:\n",
    "    # Batasi judul untuk efisiensi token\n",
    "    safe = [it for it in items if it.get(\"title\")]\n",
    "    safe = safe[:max_titles]\n",
    "    titles_block = \"\\n\".join([f\"- {it['title']} ({it.get('Url','')})\" for it in safe])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the main developments on {day_iso} from source: {source}, based on the news titles below.\n",
    "Write the output in **English**. Return exactly two sections in Markdown:\n",
    "\n",
    "### Summary\n",
    "[3–5 concise sentences in English]\n",
    "\n",
    "### Highlights\n",
    "- [3–6 short bullet points with facts/names/numbers in English.]\n",
    "\n",
    "Titles:\n",
    "{titles_block}\n",
    "\"\"\".strip()\n",
    "\n",
    "    last_err = None\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            r = client.chat.completions.create(\n",
    "                model=deployment,\n",
    "                messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=1.0,\n",
    "            )\n",
    "            text = r.choices[0].message.content.strip()\n",
    "            parts = text.split(\"### Highlights\")\n",
    "            summary_md   = parts[0].replace(\"### Summary\",\"\").strip()\n",
    "            highlights_md = (\"### Highlights\" + parts[1]).strip() if len(parts) > 1 else \"\"\n",
    "            return summary_md, highlights_md\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(backoff * (i+1))\n",
    "    return (\"Summary unavailable due to API error.\",\n",
    "            f\"### Highlights\\n- Error: {last_err}\")\n",
    "    \n",
    "# -------------------------------------------------\n",
    "# 5) Kelompokkan per source & hindari duplikasi\n",
    "#    (jangan tulis jika sudah ada untuk tanggal tsb)\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Ambil existing summary utk tanggal tsb (untuk anti-duplikat)\n",
    "existing = (\n",
    "    spark.read.jdbc(JDBC_URL, TABLE_SUMMARY, properties=JDBC_PROPS)\n",
    "         .filter(F.col(\"summary_date\") == F.to_date(F.lit(p_date)))\n",
    "         .select(\"source_name\")\n",
    "         .distinct()\n",
    "         .toPandas()\n",
    ")\n",
    "already = set((existing[\"source_name\"].fillna(\"\")).tolist()) if len(existing)>0 else set()\n",
    "\n",
    "grouped = (\n",
    "    articles_df.groupBy(\"source_name\")\n",
    "               .agg(F.collect_list(F.struct(\"title\",\"Url\")).alias(\"items\"),\n",
    "                    F.count(\"*\").alias(\"cnt\"))\n",
    "               .collect()\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for row in grouped:\n",
    "    src = row[\"source_name\"] or \"Unknown\"\n",
    "    cnt = int(row[\"cnt\"])\n",
    "    if cnt < p_min_titles:\n",
    "        continue\n",
    "    if src in already:\n",
    "        # sudah ada summary utk tanggal ini & source ini\n",
    "        continue\n",
    "\n",
    "    items = [{\"title\": it[\"title\"], \"Url\": it[\"Url\"]} for it in row[\"items\"] if it[\"title\"]]\n",
    "    if not items:\n",
    "        continue\n",
    "\n",
    "    summary_md, highlights_md = summarize_batch(src, items, p_date)\n",
    "    rows.append((p_date, src, cnt, summary_md, highlights_md))\n",
    "\n",
    "print(f\"Prepared {len(rows)} summaries (new only)\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6) Tulis hasil ke tabel DWH\n",
    "# ------------------------------\n",
    "if rows:\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"summary_date\",   T.StringType(),  False),\n",
    "        T.StructField(\"source_name\",    T.StringType(),  True),\n",
    "        T.StructField(\"articles_count\", T.IntegerType(), False),\n",
    "        T.StructField(\"summary_md\",     T.StringType(),  False),\n",
    "        T.StructField(\"highlights_md\",  T.StringType(),  True),\n",
    "    ])\n",
    "    df_out = spark.createDataFrame([Row(*x) for x in rows], schema) \\\n",
    "                  .withColumn(\"summary_date\", F.to_date(\"summary_date\")) \\\n",
    "                  .withColumn(\"created_ts\", F.current_timestamp())\n",
    "\n",
    "    (df_out\n",
    "        .select(\"summary_date\",\"source_name\",\"articles_count\",\"summary_md\",\"highlights_md\",\"created_ts\")\n",
    "        .write.mode(\"append\")\n",
    "        .jdbc(JDBC_URL, TABLE_SUMMARY, properties=JDBC_PROPS))\n",
    "\n",
    "    print(f\"Inserted {df_out.count()} rows into {TABLE_SUMMARY} for {p_date}\")\n",
    "else:\n",
    "    print(\"No new summaries to insert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "157ca8aa-0836-4f08-94e9-2ef9b487662a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "create_news_summary_with_azure_openai",
   "widgets": {
    "p_date": {
     "currentValue": "",
     "nuid": "cbede0f5-d59e-429f-8972-3ee06931df78",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "p_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "p_date",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "p_min_titles": {
     "currentValue": "1",
     "nuid": "19598de9-5ef7-4ac2-b71d-5f891ca253fe",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1",
      "label": null,
      "name": "p_min_titles",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "1",
      "label": null,
      "name": "p_min_titles",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
