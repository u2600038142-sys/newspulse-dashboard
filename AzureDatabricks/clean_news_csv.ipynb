{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b0c7d7-b6bb-4215-a088-3639f9fd6b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===== Params (diisi ADF saat run) =====\n",
    "dbutils.widgets.text(\"p_date\",   \"2025-08-29\")\n",
    "dbutils.widgets.text(\"p_source\", \"reuters\")\n",
    "\n",
    "p_date   = dbutils.widgets.get(\"p_date\")\n",
    "p_source = dbutils.widgets.get(\"p_source\")\n",
    "\n",
    "# ===== Storage & SAS =====\n",
    "storage_acct = \"stnewspulsedev\"            # ganti punyamu\n",
    "container    = \"raw\"\n",
    "sas_token    = \"sv=2024-11-04&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2025-09-30T14:36:51Z&st=2025-08-30T06:21:51Z&spr=https&sig=6EaBFeIGtbbRo95e8rZm4MNrlrKhPrz7k7pxjA%2BykpY%3D\"            # PASTE SAS token (mulai dari tanda ?)\n",
    "\n",
    "# Simpan SAS di Spark config (format kuncinya HARUS persis begini)\n",
    "spark.conf.set(f\"fs.azure.sas.{container}.{storage_acct}.blob.core.windows.net\", sas_token)\n",
    "\n",
    "# Path WASBS (Blob endpoint). Kalau pakai ADLS Gen2 + OAuth, nanti bisa ganti ke abfss://\n",
    "in_path  = f\"wasbs://{container}@{storage_acct}.blob.core.windows.net/ingest_date={p_date}/source={p_source}/*.csv\"\n",
    "tmp_path = f\"wasbs://{container}@{storage_acct}.blob.core.windows.net/__tmp/ingest_date={p_date}/source={p_source}\"\n",
    "out_path = f\"wasbs://{container}@{storage_acct}.blob.core.windows.net/clean/ingest_date={p_date}/source={p_source}\"\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Baca semua baris sebagai TEXT (kolom 'value')\n",
    "raw_lines = spark.read.text(in_path)\n",
    "\n",
    "# 2) Filter baris yang terlihat CSV valid\n",
    "#looks_like_csv = (\n",
    "#    (F.size(F.split(F.col(\"value\"), \",\")) >= 3)    # ada â‰¥3 kolom dipisah koma\n",
    "#    & (~F.col(\"value\").rlike(r\"^-{3,}\"))           # buang boundary -----\n",
    "#    & (~F.col(\"value\").rlike(r\"^Content-\"))        # buang header multipart\n",
    "#    & (F.length(F.trim(F.col(\"value\"))) > 0)       # buang baris kosong\n",
    "#)\n",
    "looks_like_csv = (\n",
    "    (~F.col(\"value\").rlike(r\"^-{3,}\"))           # buang boundary -----\n",
    "    & (~F.col(\"value\").rlike(r\"^Content-\"))        # buang header multipart\n",
    "    & (F.length(F.trim(F.col(\"value\"))) > 0)       # buang baris kosong\n",
    ")\n",
    "clean_lines = raw_lines.where(looks_like_csv)\n",
    "\n",
    "# 3) Tulis sementara sebagai text (1 baris/record) supaya bisa diparse ulang sebagai CSV\n",
    "clean_lines.coalesce(1).write.mode(\"overwrite\").text(tmp_path)\n",
    "\n",
    "# Baca kembali sebagai CSV valid\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "clean_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)      # baris pertama di tmp jadi header\n",
    "         .option(\"quote\", '\"')        # dukung koma di dalam judul\n",
    "         .option(\"escape\", '\"')\n",
    "         .option(\"multiLine\", True)   # jaga-jaga ada newline di dalam kutip\n",
    "         .csv(tmp_path)\n",
    "         .select(\"Title\",\"Url\")\n",
    ")\n",
    "\n",
    "# Tambahkan ingest_date biar enak dipakai di SQL\n",
    "clean_df = clean_df.withColumn(\"source_name\", F.lit(\"Reuters\"))\n",
    "clean_df = clean_df.withColumn(\"ingest_date\", F.to_date(F.lit(p_date)))\n",
    "\n",
    "import os\n",
    "\n",
    "# 1. Tulis seperti biasa\n",
    "(clean_df\n",
    "    .coalesce(1)\n",
    "    .write.mode(\"overwrite\")\n",
    "    .option(\"header\", True)\n",
    "    .csv(out_path))\n",
    "\n",
    "# 2. Cari file part-*.csv\n",
    "files = dbutils.fs.ls(out_path)\n",
    "csv_file = [f.path for f in files if f.path.endswith(\".csv\")][0]\n",
    "\n",
    "# 3. Tentukan nama baru\n",
    "target_path = os.path.join(out_path, \"clean_reuters.csv\")\n",
    "\n",
    "# 4. Rename\n",
    "dbutils.fs.mv(csv_file, target_path)\n",
    "\n",
    "print(\"Clean CSV renamed to:\", target_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "clean_news_csv",
   "widgets": {
    "p_date": {
     "currentValue": "2025-08-29",
     "nuid": "06cf7be4-ae93-4e6b-943e-eb7206099f43",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-08-29",
      "label": null,
      "name": "p_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2025-08-29",
      "label": null,
      "name": "p_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "p_source": {
     "currentValue": "reuters",
     "nuid": "b2b16681-d0a6-4c74-8417-7347e41650c5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "reuters",
      "label": null,
      "name": "p_source",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "reuters",
      "label": null,
      "name": "p_source",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
